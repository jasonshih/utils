#!/bin/bash

if [ $# -lt 1 ]; then
   echo "Usage:
		$0 hadoop-release-package.tgz

		e.g.:

		if aiming to replace current HDFS release:
		$0 hadoop-2.0.0-cdh4.2.0.tar.gz 

		or if updating version of HBase deployed on EA:
		$0 hbase-0.94.2-cdh4.2.0.tar.gz
"
exit 
fi


if [ $UID != 0 ]; then
   echo "root privilege required!" 
   exit 1
fi


# common func
info() {
    echo -e "`date +%c` - INFO: $* \n"
}

warn() {
    echo -e "`date +%c` - WARN: $* \n"
}

error() {
    printf  "`date +%c` - ERROR: $*\n" 
    exit 1
}

chk_file () {
  if [ ! -f $1 ]; then
     error "Missing file: $1"
     exit 1
  fi
}

chk_dir () {
  if [ ! -d $1 ]; then
     error "Cant find directory: $1"
     return 1
  fi
}


_get_pkgname () {

  _n=`echo $1 | awk -F'-' '{print $1}'`
  if [ `echo $1 | grep mr1 >/dev/null ; echo $?` == 0 ]; then
     _n="hadoopmr"
  fi
  echo $_n

}


# vars
_bkp_root="/var/disk/a/user/etu/.backup"
_pkg_root="/opt"
_pkg_name=`_get_pkgname $1`
_pkg_home="$_pkg_root/$_pkg_name"
etu_cfg="/opt/etu/etc/config.properties"
info "Updating package $_pkg_name on EA cluster now"
if [ `which fping >/dev/null; echo $?` != 0 ]; then
   _ping="ping -c1 -w2"
else
   _ping="fping -c1 -t100"
fi


_chk_sys () {

   chk_file $1
   chk_dir $_pkg_home
   if [ $? != 0 ]; then
      error "Cant find matching package name $_pkg_name from default deployment root $_pkg_root"
   fi

}


backup_pkg () {

   chk_dir $_bkp_root
   # preserve backup for master only
   info "Backing up package $_pkg_name settings"
   #cp -r $_pkg_home/etc/hadoop $_bkp_root
   cp -r $_pkg_home $_bkp_root

}


_cleanup_pkg () {

    if [ $# -lt 1 ]; then
        echo "Usage: $0 pkg-name"
        exit 1
    elif [ ! -d $1 ]; then
        echo "ERROR: missing package directory: $1"
        exit 1
    fi  

    cd $1

    # seeking upto 2nd deepth
    find -maxdepth 1 -type d -ls \
        | grep -E 'cloudera|contrib|dist-maven|distribution|docs|example-confs|examples|integration|ivy|lib-src|license|math|recipes|sqoop-test|src|taste-web|test|testdata|tutorial|utils' \
        | awk '{print $(NF)}' \
        | xargs rm -fr 

        # js: force purging all docs folder if hide in subfolder, previous cleanup limit to first layer only 
        #     hadoop share/doc size around 63M.
        find . -name doc -type d | xargs rm -fr 


    # purge unnecessary files from raw pkg
    find -maxdepth 1 -type f -ls \
        | grep -E 'build.properties|build.xml|cdh.build.properties|CHANGELOG.txt|CHANGES.txt|cloudera-pom.xml|DISCLAIMER.txt|COMPILING.txt|pom-old.xml|pom.xml|README_packaging.txt|README.txt|ivysettings.xml|ivy.xml|LICENSE|LICENSE.txt|NOTICE|NOTICE.txt|RELEASE_NOTES.txt' \
        | awk '{print $(NF)}' \
        | xargs rm -f

}


update_pkg () {

   warn "Purge existing $_pkg_name application"
   rm -fr $_pkg_home/*
   if [ `echo $1 | grep "tar.gz" >/dev/null; echo $?` == 0 ]; then
      _pkg_tmp=`basename $1 .tar.gz`
   elif [ `echo $1 | grep "tgz" >/dev/null ; echo $?` == 0 ]; then
      _pkg_tmp=`basename $1 .tgz`
   else
      warn "file extension not recognized (not tar.gz or tgz)"
      exit 1
   fi
   info "Extracting package installaition tarball into $_pkg_home"
   tar zxf $1 -C $_pkg_home
   if [ -d $_pkg_home/$_pkg_tmp ]; then
      mv $_pkg_home/$_pkg_tmp/* $_pkg_home/
      info "Purge unnecessary files from $_pkg_name pkg"
      _cleanup_pkg $_pkg_home
      #rm -fr $_pkg_home/src $_pkg_home/cloudera $_pkg_home/share/doc $_pkg_home/examples
      rm -fr $_pkg_home/$_pkg_tmp
   else
      if [ `cd $_pkg_home; ls | wc -l` -gt 1 ]; then
         warn "More than one folder found in $_pkg_home"
         warn "leave unchange of current folder structure"
         exit 1
      fi
      _pkg_tmp=`cd $_pkg_home; ls`
      mv $_pkg_home/$_pkg_tmp/* $_pkg_home/ 
      info "Purge unnecessary files from $_pkg_name pkg"
      _cleanup_pkg $_pkg_home
      rm -fr $_pkg_home/$_pkg_tmp
   fi
   # restore cfg - skip, force restarting namenode will override existing settings
   #cp -r $_bkp_root/hadoop/etc/hadoop $_pkg_home/etc/hadoop
   if [ $_pkg_name == "hadoop" ]; then
      touch $_pkg_home/etc/hadoop/etu_worker_decommission
      cp /opt/etu/libexec/jsvc $_pkg_home/libexec
   fi
   chown -R etu.etu $_pkg_home

}


load_var () {

  if [ ! -f $etu_cfg ]; then
     error "Cant find Etu config properties"
     exit 1
  fi
  source  $etu_cfg
  _master=$USER_HOST_NAME
  _master_ha=$USER_HOST_NAME_STANDBY_MASTER
  _master_fqdn=$USER_HOST_NAME.$USER_DOMAIN
  _dn=$USER_DOMAIN
  _krb_dn=`echo $USER_DOMAIN | tr [a-z] [A-Z]`
  etu_princ="etu/_HOST@$_krb_dn"

}


sync_pkg () {

  for i in `_get_worker`
  do
     info "syncing package to worker $i"
     ssh $i "cd $_pkg_home ; rm -fr * ; scp -r $_master:$_pkg_home/* . ; chown -R etu.etu $_pkg_home"
  done

}


_get_worker () {

  load_var
  #info "Get all Etu worker host list"
  _th=(`grep $_dn /etc/hosts | awk '{print $(NF)}' | grep -v $_master | grep -v $_master_ha`)
  #info "Exclude offline workers from defined workers"
  let j=0
  for i in ${_th[@]}
  do
     $_ping $i >& /dev/null
     if [ $? != 0 ]; then
        #info "Removing $i from host list (offline or unreachable"
	unset _th[$j]
     fi
     let j+=1
  done
  echo ${_th[@]}

}


_mod_name () {

   case "$1" in
   
   hadoop)
       echo "HadoopDfsModule"
       ;;
   hadoopmr)
       echo "HadoopMrModule"
       ;;
   hbase)
       echo "HBaseModule"
       ;;
   *)
       echo ""
       ;;
   esac

}


restart_mod () {

  local _etu_mod=`_mod_name $_pkg_name`

  if [ -z $_etu_mod ]; then
     info "Cant find defined Etu module to performance service restart"
     exit 
  else

     echo -n "Etu web management console password: " 
     read -s ETU_ADMPASS
     until ! [[ "y$ETU_ADMPASS" = "y" ]]
     do
        echo -n "Etu web management console password: " 
        read -s ETU_ADMPASS
     done
   
     local adm_acc=admin
     local _cache=$HOME/.header
     local _curl="curl --insecure"
     #local _etu_mod="HadoopDfsModule"
     local etu_web="https://$_master_fqdn"
     local adm_pass="${ETU_ADMPASS}" # default password for etu web admin
     info "Restart $_pkg_name service from Etu management console"
     $_curl -d "user=$adm_acc&password=$adm_pass" --dump-header $_cache "$etu_web/login.html"
     $_curl -d "service=$_etu_mod" -L -b $_cache "$etu_web/module/stopService.do"
     sleep 2
     $_curl -d "service=$_etu_mod" -L -b $_cache "$etu_web/module/startService.do"
     #curl --insecure -L -b $_cache "https://$_master_fqdn/module/serviceList.do"  | python -mjson.tool |grep -E 'serviceFullName|serviceName|serviceStatus'
     echo ""
     info "Cleanup web cache"
     rm -f $_cache
  fi

}



# main
_chk_sys $1
load_var
backup_pkg
update_pkg $1
sync_pkg
load_var
restart_mod

# end
